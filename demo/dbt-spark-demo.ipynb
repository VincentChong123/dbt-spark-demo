{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b257ecab",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Demo of testing ETL pipeline using DBT and spark\"\n",
    "date: last-modified\n",
    "format:\n",
    "  html:\n",
    "    toc: true\n",
    "    toc-depth: 4\n",
    "    toc-location: left    \n",
    "    code-fold: true\n",
    "    css: styles.css  \n",
    "    number-sections: true       \n",
    "    other-links:\n",
    "      - text: github.com project repo\n",
    "        href: https://github.com/VincentChong123/dbt-spark-demo.git\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5888a439",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Introduction\n",
    "This is a demo of dbt work flow using an data ingestion as an example. In finance data ETL (Extract, Transform, Load)\n",
    "important to have sanitity check of input data, to avoid the ingesting invalid data...."
   ]
  },
  {
   "cell_type": "raw",
   "id": "e897f7e7",
   "metadata": {
    "tags": [
     "skip"
    ],
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```{mermaid}\n",
    "flowchart LR\n",
    "   empty[\".\"] --> DBT[\"Container: dbt-core \n",
    "       plugin: dbt-\n",
    "  DBT-pyspark[Hive]\"] --> |Generate SQL with 'dbt seed', 'dbt compile'| T(Container: Spark-Thrift)\n",
    "  T -->|Query using the generated SQL| Storage[(Container: Postgre Hive metadata store)]\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae9c7095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>amount</th>\n",
       "      <th>currency</th>\n",
       "      <th>status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>USD</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>SGD</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>EUR</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>120</td>\n",
       "      <td>USD</td>\n",
       "      <td>INACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>180</td>\n",
       "      <td>SGD</td>\n",
       "      <td>INACTIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>300</td>\n",
       "      <td>JPY</td>\n",
       "      <td>ACTIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  amount currency    status\n",
       "0   1     100      USD    ACTIVE\n",
       "1   2     200      SGD    ACTIVE\n",
       "2   3     150      EUR    ACTIVE\n",
       "3   4     120      USD  INACTIVE\n",
       "4   5     180      SGD  INACTIVE\n",
       "5   6     300      JPY    ACTIVE"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_csv(\"/usr/app/demo/dbt_spark_demo_prj/seeds/transactions/raw__transactions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8640c45",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Project file structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b28d4077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m/usr/app\u001b[00m\n",
      "├── \u001b[01;34mdbt\u001b[00m\n",
      "│   ├── dbt.Dockerfile\n",
      "│   ├── \u001b[01;34mdbt_spark_demo_prj\u001b[00m\n",
      "│   ├── docker-compose.yml\n",
      "│   └── requirements.txt\n",
      "├── \u001b[01;34mdbt-spark\u001b[00m\n",
      "│   ├── \u001b[01;34mdagger\u001b[00m\n",
      "│   ├── \u001b[01;34mdbt\u001b[00m\n",
      "│   ├── \u001b[01;34mdocker\u001b[00m\n",
      "│   ├── docker-compose.yml\n",
      "│   ├── \u001b[01;34mscripts\u001b[00m\n",
      "│   ├── test.env.example\n",
      "│   └── \u001b[01;34mtests\u001b[00m\n",
      "├── \u001b[01;34mdemo\u001b[00m\n",
      "│   ├── dbt-spark-demo.html\n",
      "│   ├── dbt-spark-demo.ipynb\n",
      "│   ├── \u001b[01;34mdbt-spark-demo_files\u001b[00m\n",
      "│   ├── \u001b[01;34mdbt_spark_demo_prj\u001b[00m\n",
      "│   ├── generate-documentation.sh\n",
      "│   └── profiles.yml\n",
      "└── docker-common.sh\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!tree -L 2 --sort=\"name\" /usr/app | grep -v \".*\\.toml$\" | grep -v \".*\\.md$\" | grep -v \".*files$\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13bae7",
   "metadata": {},
   "source": [
    "# Environment setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0152385",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Docker\n",
    "### Shared network for docker containers\n",
    "```bash\n",
    "source ./docker-common.sh\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de346d1",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Install jupyter notebook for running this demo in notebook \n",
    "This notebook is executed using jupyer-notebook kernel (http://127.0.0.1:8888) of the loaded custom-dbt container. The container is loaded using command below\n",
    "```bash\n",
    "jupyter notebook --NotebookApp.token='' --NotebookApp.password='' --NotebookApp.disable_check_xsrf=True --allow-root --ip=0.0.0.0 --port=8888 --no-browser\n",
    "```\n",
    "use allow-root for demo purpose only, not for production due to cyber security\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a58c3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Jun 29 23:09:16 +08 2025\n",
      "/usr/local/bin/python\n",
      "/usr/app/demo\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export TZ='Asia/Singapore'\n",
    "date\n",
    "which python\n",
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e7c606",
   "metadata": {},
   "source": [
    "### Connection to read table in Hive meta-data store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b175ae",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f86badf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/app/demo\n",
      "/usr/app/demo\n",
      "dbt-spark3-thrift\n",
      "/usr/app/dbt/dbt_spark_demo_prj\n",
      "dbt_spark_demo_prj\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "!pwd\n",
    "\n",
    "!cd ../\n",
    "!pwd\n",
    "\n",
    "load_dotenv()\n",
    "spark_thrift_service_name=os.getenv('SPARK_CONTAINER_SERVICE_NAME')\n",
    "dbt_project_dir=os.getenv('DBT_PROJECT_DIR')\n",
    "dbt_project_name=os.getenv('DBT_PROJECT_NAME')\n",
    "print(spark_thrift_service_name)\n",
    "print(dbt_project_dir)\n",
    "print(dbt_project_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f0f26b",
   "metadata": {},
   "source": [
    "## Helper codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9f4df75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>raw__transactions</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>default</td>\n",
       "      <td>transactions</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  namespace          tableName  isTemporary\n",
       "0   default  raw__transactions        False\n",
       "1   default       transactions        False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n"
     ]
    }
   ],
   "source": [
    "from pyhive import hive\n",
    "import pandas as pd\n",
    "from sqlalchemy.engine import create_engine\n",
    "\n",
    "def check_table(host=spark_thrift_service_name, message_to_be_printed=\"\"):\n",
    "    # with hive.Connection(host=spark_thrift_service_name, port=10000, \n",
    "    #                     #    username='', \n",
    "    #                     database='default') as conn:\n",
    "\n",
    "    #     # Query all tables in the default database\n",
    "    #     df = pd.read_sql(\"SHOW TABLES\", conn)\n",
    "\n",
    "    #     # Display tables\n",
    "    #     display(df)\n",
    "    print(message_to_be_printed)\n",
    "    engine = create_engine(f'hive://{spark_thrift_service_name}:10000/default')\n",
    "    df = pd.read_sql(\"SHOW TABLES;\", engine)\n",
    "    # display(df)\n",
    "    display(df)\n",
    "\n",
    "def show_table_details(table_name, database_name='default',\n",
    "                        # host=spark_thrift_service_name,\n",
    "                        port=10000):\n",
    "    # sqlAlchemy api fail here due to \"sqlAlchemy relection\": TODO investigate root cause\n",
    "    # engine = create_engine(f'hive://{spark_thrift_service_name}:10000/default')\n",
    "    # df = pd.read_sql(f\"select * from {table_name} limit 10\", engine)\n",
    "\n",
    "    with hive.Connection(host=spark_thrift_service_name, port=port, database=database_name) as conn:\n",
    "        df = pd.read_sql(f\"select * from {database_name}.{table_name}\", conn)\n",
    "\n",
    "        display(df)\n",
    "\n",
    "    \n",
    "check_table()\n",
    "print(\"------------\")\n",
    "# show_table_details(\"transactions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ca1b42",
   "metadata": {},
   "source": [
    "## DBT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf3c236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core:\n",
      "  - installed: 1.10.2\n",
      "  - latest:    1.10.2 - \u001b[32mUp to date!\u001b[0m\n",
      "\n",
      "Plugins:\n",
      "  - spark: 1.9.2 - \u001b[32mUp to date!\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!dbt --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe04595",
   "metadata": {},
   "source": [
    "## Verify DBT setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90d22f",
   "metadata": {},
   "source": [
    "### profiles.yml\n",
    "```yml\n",
    "formal_verification_prj_name:\n",
    "  outputs:\n",
    "    spark:\n",
    "      type: spark\n",
    "      method: thrift\n",
    "      host: dbt-spark3-thrift\n",
    "      schema: default\n",
    "      connect_timeout: 30\n",
    "      authentication: NONE\n",
    "  target: spark\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70bfe66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "send_anonymous_usage_stats: false\n",
      "\n",
      "dbt_spark_demo_prj:\n",
      "  outputs:\n",
      "    spark:\n",
      "      type: spark\n",
      "      method: thrift\n",
      "      host: dbt-spark3-thrift\n",
      "      port: 10000\n",
      "      schema: default\n",
      "      connect_timeout: 30\n",
      "      authentication: NONE\n",
      "  target: spark\n"
     ]
    }
   ],
   "source": [
    "!cat ~/.dbt/profiles.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c21787",
   "metadata": {},
   "source": [
    "### dbt_project.yml\n",
    "```yml\n",
    "name: 'formal_verification_prj_name'\n",
    "version: '1.0.0'\n",
    "\n",
    "# This setting configures which \"profile\" dbt uses for this project.\n",
    "profile: 'formal_verification_prj_name'\n",
    "\n",
    "# These configurations specify where dbt should look for different types of files.\n",
    "# The `model-paths` config, for example, states that models in this project can be\n",
    "# found in the \"models/\" directory. You probably won't need to change these!\n",
    "model-paths: [\"models\"]\n",
    "analysis-paths: [\"analyses\"]\n",
    "test-paths: [\"tests\"]\n",
    "seed-paths: [\"seeds\"]\n",
    "macro-paths: [\"macros\"]\n",
    "snapshot-paths: [\"snapshots\"]\n",
    "\n",
    "clean-targets:         # directories to be removed by `dbt clean`\n",
    "  - \"target\"\n",
    "  - \"dbt_packages\"\n",
    "\n",
    "models:\n",
    "  formal_verification_prj_name:\n",
    "    # +write_json: false\n",
    "    +materialized: incremental\n",
    "    +file_format: parquet    \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "089c0b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# Name your project! Project names should contain only lowercase characters\n",
      "# and underscores. A good package name should reflect your organization's\n",
      "# name or the intended use of these models\n",
      "name: 'dbt_spark_demo_prj'\n",
      "version: '1.0.0'\n",
      "\n",
      "# This setting configures which \"profile\" dbt uses for this project.\n",
      "profile: 'dbt_spark_demo_prj'\n",
      "\n",
      "# These configurations specify where dbt should look for different types of files.\n",
      "# The `model-paths` config, for example, states that models in this project can be\n",
      "# found in the \"models/\" directory. You probably won't need to change these!\n",
      "model-paths: [\"models\"]\n",
      "analysis-paths: [\"analyses\"]\n",
      "test-paths: [\"tests\"]\n",
      "seed-paths: [\"seeds\"]\n",
      "macro-paths: [\"macros\"]\n",
      "snapshot-paths: [\"snapshots\"]\n",
      "\n",
      "clean-targets:         # directories to be removed by `dbt clean`\n",
      "  - \"target\"\n",
      "  - \"dbt_packages\"\n",
      "\n",
      "\n",
      "# Configuring models\n",
      "# Full documentation: https://docs.getdbt.com/docs/configuring-models\n",
      "\n",
      "# In this example config, we tell dbt to build all models in the example/\n",
      "# directory as views. These settings can be overridden in the individual model\n",
      "# files using the `{{ config(...) }}` macro.\n",
      "models:\n",
      "  dbt_spark_demo_prj:\n",
      "    +materialized: incremental\n",
      "    +file_format: parquet\n",
      "  # example:\n",
      "  #   +enabled: false\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "!cat /usr/app/dbt/dbt_spark_demo_prj/dbt_project.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa3d336",
   "metadata": {},
   "source": [
    "### dbt debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "463ce0e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m15:09:26  Running with dbt=1.10.2\n",
      "\u001b[0m15:09:26  dbt version: 1.10.2\n",
      "\u001b[0m15:09:26  python version: 3.11.2\n",
      "\u001b[0m15:09:26  python path: /usr/local/bin/python3\n",
      "\u001b[0m15:09:26  os info: Linux-5.15.167.4-microsoft-standard-WSL2-x86_64-with-glibc2.31\n",
      "\u001b[0m15:09:26  Using profiles dir at /usr/app/demo\n",
      "\u001b[0m15:09:26  Using profiles.yml file at /usr/app/demo/profiles.yml\n",
      "\u001b[0m15:09:26  Using dbt_project.yml file at /usr/app/dbt/dbt_spark_demo_prj/dbt_project.yml\n",
      "\u001b[0m15:09:26  adapter type: spark\n",
      "\u001b[0m15:09:26  adapter version: 1.9.2\n",
      "\u001b[0m15:09:26  Configuration:\n",
      "\u001b[0m15:09:26    profiles.yml file [\u001b[32mOK found and valid\u001b[0m]\n",
      "\u001b[0m15:09:26    dbt_project.yml file [\u001b[32mOK found and valid\u001b[0m]\n",
      "\u001b[0m15:09:26  Required dependencies:\n",
      "\u001b[0m15:09:26   - git [\u001b[32mOK found\u001b[0m]\n",
      "\n",
      "\u001b[0m15:09:26  Connection:\n",
      "\u001b[0m15:09:26    host: dbt-spark3-thrift\n",
      "\u001b[0m15:09:26    port: 10000\n",
      "\u001b[0m15:09:26    cluster: None\n",
      "\u001b[0m15:09:26    endpoint: None\n",
      "\u001b[0m15:09:26    schema: default\n",
      "\u001b[0m15:09:26    organization: 0\n",
      "\u001b[0m15:09:26  Registered adapter: spark=1.9.2\n",
      "\u001b[0m15:09:26    Connection test: [\u001b[32mOK connection ok\u001b[0m]\n",
      "\n",
      "\u001b[0m15:09:26  \u001b[32mAll checks passed!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!dbt debug --project-dir /usr/app/dbt/dbt_spark_demo_prj\n",
    "\n",
    "#to print detailed messages for debugging \n",
    "#!dbt debug --debug --project-dir /usr/app/dbt/dbt_spark_demo_prj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6788bb54",
   "metadata": {},
   "source": [
    "## dbt command comparisons\n",
    "| Command           | Purpose                                  | What it does                                                             | Output                                    |\n",
    "| ----------------- | ---------------------------------------- | ------------------------------------------------------------------------ | ----------------------------------------- |\n",
    "| **`dbt compile`** | Prepares SQL code for execution          | Converts Jinja + macros into raw SQL; writes compiled files to `target/` | Compiled SQL files (no warehouse changes) |\n",
    "| **`dbt run`**     | Builds models (tables/views)             | Executes compiled SQL to materialize models in the data warehouse        | Tables/views in the warehouse             |\n",
    "| **`dbt test`**    | Validates data quality assumptions       | Runs data tests (e.g., `not_null`, `unique`, custom) on warehouse data   | Test pass/fail results                    |\n",
    "| **`dbt seed`**    | Loads static CSV data into the warehouse | Uploads CSV files from the `seeds/` directory into tables                 | Tables containing seed data               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7b25b",
   "metadata": {},
   "source": [
    "# Prepare SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b5522",
   "metadata": {},
   "source": [
    "## Read dbt input csv \n",
    "\n",
    "* read seeds/transactions.csv, \n",
    "* dbt generate raw__transaction.csv\n",
    "* dbt create table default.raw_transaction at the database "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331b109e",
   "metadata": {},
   "source": [
    "### Verify database before the operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c4e3091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m15:09:31  Running with dbt=1.10.2\n",
      "\u001b[0m15:09:31  Registered adapter: spark=1.9.2\n",
      "\u001b[0m15:09:32  Found 1 model, 1 test, 1 seed, 476 macros, 3 unit tests\n",
      "\u001b[0m15:09:37  Running with dbt=1.10.2\n",
      "\u001b[0m15:09:37  Registered adapter: spark=1.9.2\n",
      "\u001b[0m15:09:38  Found 1 model, 1 test, 1 seed, 476 macros, 3 unit tests\n"
     ]
    }
   ],
   "source": [
    "!dbt run-operation drop_table --args '{\"table_name\": \"default.raw__transactions\"}'  \n",
    "!dbt run-operation drop_table --args '{\"table_name\": \"default.transactions\"}' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff9ca4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database table before the operation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [namespace, tableName, isTemporary]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m15:09:43  Running with dbt=1.10.2\n",
      "\u001b[0m15:09:44  Registered adapter: spark=1.9.2\n",
      "\u001b[0m15:09:44  Found 1 model, 1 test, 1 seed, 476 macros, 3 unit tests\n",
      "\u001b[0m15:09:44  \n",
      "\u001b[0m15:09:44  Concurrency: 1 threads (target='spark')\n",
      "\u001b[0m15:09:44  \n",
      "\u001b[0m15:09:44  1 of 1 START seed file default.raw__transactions ............................... [RUN]\n",
      "\u001b[0m15:09:45  1 of 1 OK loaded seed file default.raw__transactions ........................... [\u001b[32mINSERT 6\u001b[0m in 0.70s]\n",
      "\u001b[0m15:09:45  \n",
      "\u001b[0m15:09:45  Finished running 1 seed in 0 hours 0 minutes and 0.98 seconds (0.98s).\n",
      "\u001b[0m15:09:45  \n",
      "\u001b[0m15:09:45  \u001b[32mCompleted successfully\u001b[0m\n",
      "\u001b[0m15:09:45  \n",
      "\u001b[0m15:09:45  Done. PASS=1 WARN=0 ERROR=0 SKIP=0 NO-OP=0 TOTAL=1\n",
      "\n",
      "Database table after the operation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>raw__transactions</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  namespace          tableName  isTemporary\n",
       "0   default  raw__transactions        False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_table(message_to_be_printed=\"Database table before the operation\")\n",
    "!dbt seed --log-level info --project-dir $dbt_project_dir\n",
    "check_table(message_to_be_printed=\"\\nDatabase table after the operation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99518976",
   "metadata": {},
   "source": [
    "## dbt compile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299472eb",
   "metadata": {},
   "source": [
    "#### removed target/compiled/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be2451a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'dbt_spark_demo_prj/target/compile': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!rm -rf dbt_spark_demo_prj/target/compiled/\n",
    "!ls dbt_spark_demo_prj/target/compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6d17db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dbt_project.yml  logs  macros  models  seeds  target  tests\n"
     ]
    }
   ],
   "source": [
    "!ls $dbt_project_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7789867d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m15:09:52  Running with dbt=1.10.2\n",
      "\u001b[0m15:09:52  Registered adapter: spark=1.9.2\n",
      "\u001b[0m15:09:53  Found 1 model, 1 test, 1 seed, 476 macros, 3 unit tests\n",
      "\u001b[0m15:09:53  \n",
      "\u001b[0m15:09:53  Concurrency: 1 threads (target='spark')\n",
      "\u001b[0m15:09:53  \n"
     ]
    }
   ],
   "source": [
    "!dbt compile --log-level info --project-dir $dbt_project_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2075b987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models\ttests\n"
     ]
    }
   ],
   "source": [
    "!ls /usr/app/dbt/dbt_spark_demo_prj/target/compiled/dbt_spark_demo_prj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c8f3a",
   "metadata": {},
   "source": [
    "## Build your models (dbt run)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e5d338",
   "metadata": {},
   "source": [
    "## Delete database table prior dbt run that will create the table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49dad449",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>raw__transactions</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  namespace          tableName  isTemporary\n",
       "0   default  raw__transactions        False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m15:09:59  Running with dbt=1.10.2\n",
      "\u001b[0m15:09:59  Registered adapter: spark=1.9.2\n",
      "\u001b[0m15:10:00  Found 1 model, 1 test, 1 seed, 476 macros, 3 unit tests\n",
      "\n",
      "after\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>namespace</th>\n",
       "      <th>tableName</th>\n",
       "      <th>isTemporary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>default</td>\n",
       "      <td>raw__transactions</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  namespace          tableName  isTemporary\n",
       "0   default  raw__transactions        False"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check_table(message_to_be_printed=\"before\")\n",
    "!dbt run-operation drop_table --args '{\"table_name\": \"default.transactions\"}'\n",
    "check_table(message_to_be_printed=\"\\nafter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf648b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m15:10:05  Running with dbt=1.10.2\n",
      "\u001b[0m15:10:06  Registered adapter: spark=1.9.2\n"
     ]
    }
   ],
   "source": [
    "!rm -r $dbt_project_dir/target/run\n",
    "!dbt run-operation drop_table --args '{\"table_name\": \"default.transactions\"}' \n",
    "check_table(message_to_be_printed=\"before\")\n",
    "\n",
    "!dbt run --log-level info --project-dir $dbt_project_dir\n",
    "\n",
    "print(\"\\nAfter\")\n",
    "!tree $dbt_project_dir/target/run\n",
    "check_table(message_to_be_printed=\"after\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cf4b24",
   "metadata": {},
   "source": [
    "## test model using dbt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc3baeb",
   "metadata": {},
   "source": [
    "#### test name and its test vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dfaae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat dbt_spark_demo_prj/tests/unit/test_transactions.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31610ca8",
   "metadata": {},
   "source": [
    "#### run test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c0834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export DBT_PROJECT_DIR=/usr/app/dbt/dbt_spark_demo_prj\n",
    "dbt test --log-level info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d37f7eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export TZ='Asia/Singapore'; date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717e70b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod -R a+rw /usr/app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b840cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"Stop execution here!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebe7ae3",
   "metadata": {},
   "source": [
    "# Apendix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f84536",
   "metadata": {},
   "source": [
    "## check the model built "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57448e7",
   "metadata": {},
   "source": [
    "### Check the model built using Pyspark (In progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32863689",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f91e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# For Debian/Ubuntu-based containers:\n",
    "apt-get update && apt-get install -y openjdk-11-jdk\n",
    "\n",
    "# For Alpine-based containers:\n",
    "apk add openjdk11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c39001",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /usr/lib/jvm/java-11-openjdk-amd64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2863d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !. /usr/spark/bin/load-spark-env.sh\n",
    "\n",
    "# Typically in Debian/Ubuntu:\n",
    "!export JAVA_HOME=$(dirname $(dirname $(readlink -f $(which java))))\n",
    "\n",
    "# Or set it explicitly:\n",
    "!export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"dbt_clean\") \\\n",
    "    .config(\"spark.hadoop.hive.metastore.schema.verification\", \"true\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# spark = SparkSession.builder \\\n",
    "#     .config(\"hive.metastore.uris\", \"thrift://dbt-hive-metastore:10000\") \\\n",
    "#     .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "#     .enableHiveSupport() \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "spark.sql(\"USE default\")  # Or your specific database\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "# spark.sql(\"select * from transactions\").show()\n",
    "# # df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b054f4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW TABLES IN default\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2477376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query metastore system tables\n",
    "spark.sql(\"\"\"\n",
    "  SELECT TBL_NAME, TBL_TYPE \n",
    "  FROM default.TBLS \n",
    "  JOIN default.DBS ON TBLS.DB_ID = DBS.DB_ID \n",
    "  WHERE DBS.NAME = 'default'\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c7f91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"hive.metastore.uris\")  # Should return your URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ef3800",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.catalogImplementation\")  # Should return \"hive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91926ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark.stop()\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DBTIntegration\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://dbt-hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"javax.jdo.option.ConnectionUserName\", \"dbt\") \\\n",
    "    .config(\"javax.jdo.option.ConnectionPassword\", \"dbt\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "spark.sql(\"SHOW TABLES IN default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f49695",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark==3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34760ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyhive import hive\n",
    "# conn = hive.connect(host=spark_thrift_service_name, port=10000)\n",
    "# cursor = conn.cursor()\n",
    "# cursor.execute(\"SELECT * FROM default.transactions LIMIT 1\")\n",
    "# print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d2bd4",
   "metadata": {},
   "source": [
    "### Test the built model (using the test model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6db6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### dbt test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95bbbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!dbt test --log-level info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat /home/vin/01-prj/stripe/sql-formal-verification/formal_verification_prj_name/tests/unit/test_transactions.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c204c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# cd /usr/app/dbt\n",
    "# dbt compile --profiles-dir /usr/app/dbt  --project-dir /usr/app/dbt/dbt_spark_demo_prj\n",
    "# # dbt run-operation drop_table --args '{\"table_name\": \"default.transactions\"}'\n",
    "# dbt run-operation drop_view --args '{\"view_name\": \"default.my_first_dbt_model\"}' --profiles-dir /usr/app/dbt  --project-dir /usr/app/dbt/dbt_spark_demo_prj\n",
    "# dbt run-operation drop_view --args '{\"view_name\": \"default.my_second_dbt_model\"}' --profiles-dir /usr/app/dbt  --project-dir /usr/app/dbt/dbt_spark_demo_prj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dcb3be",
   "metadata": {},
   "source": [
    "# Todo\n",
    "## Audit compliance \n",
    "### time tracing with time-stamping\n",
    "1. Add timestamp for loaded data using dbt model\n",
    "```jinja\n",
    "SELECT\n",
    "    *,\n",
    "    '{{ run_started_at }}'::timestamp AS added_at\n",
    "FROM {{ ref('my_seed_table') }}\n",
    "```\n",
    "### File tracing with source and targets file path, tablename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf387a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
